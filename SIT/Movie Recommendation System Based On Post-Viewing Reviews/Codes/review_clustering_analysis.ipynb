{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Kenny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Kenny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/Kenny/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# add import statements\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from textblob import TextBlob\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster import KMeansClusterer, \\\n",
    "cosine_distance\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import mixture\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import text2emotion as te\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userId</th>\n",
       "      <th>reviewDate</th>\n",
       "      <th>reviewScore</th>\n",
       "      <th>reviewTitle</th>\n",
       "      <th>userReview</th>\n",
       "      <th>movieId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2509775</td>\n",
       "      <td>26 November 2003</td>\n",
       "      <td>10</td>\n",
       "      <td>Tied for the best movie I have ever seen</td>\n",
       "      <td>Why do I want to write the 234th comment on Th...</td>\n",
       "      <td>111161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1898687</td>\n",
       "      <td>10 February 2006</td>\n",
       "      <td>10</td>\n",
       "      <td>A classic piece of unforgettable film-making.</td>\n",
       "      <td>In its Oscar year, Shawshank Redemption (writt...</td>\n",
       "      <td>111161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16161013</td>\n",
       "      <td>24 July 2010</td>\n",
       "      <td>10</td>\n",
       "      <td>Some birds aren't meant to be caged.</td>\n",
       "      <td>The Shawshank Redemption is written and direct...</td>\n",
       "      <td>111161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1005460</td>\n",
       "      <td>8 February 2001</td>\n",
       "      <td>10</td>\n",
       "      <td>Prepare to be moved</td>\n",
       "      <td>I have never seen such an amazing film since I...</td>\n",
       "      <td>111161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>997166</td>\n",
       "      <td>27 August 2002</td>\n",
       "      <td>10</td>\n",
       "      <td>Shawshank Redeems Hollywood</td>\n",
       "      <td>Can Hollywood, usually creating things for ent...</td>\n",
       "      <td>111161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24183</th>\n",
       "      <td>24183</td>\n",
       "      <td>6729</td>\n",
       "      <td>17 June 2003</td>\n",
       "      <td>9</td>\n",
       "      <td>Changed my mind ...</td>\n",
       "      <td>This post refers to the extended DVD special e...</td>\n",
       "      <td>99348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24184</th>\n",
       "      <td>24184</td>\n",
       "      <td>2349843</td>\n",
       "      <td>9 May 2003</td>\n",
       "      <td>10</td>\n",
       "      <td>Read The Book</td>\n",
       "      <td>This is specifically for Tom Hull...did you ha...</td>\n",
       "      <td>99348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24185</th>\n",
       "      <td>24185</td>\n",
       "      <td>1945054</td>\n",
       "      <td>4 December 2002</td>\n",
       "      <td>10</td>\n",
       "      <td>One of the best films in the 90's</td>\n",
       "      <td>Dances with wolves is a very epic film.I must ...</td>\n",
       "      <td>99348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24186</th>\n",
       "      <td>24186</td>\n",
       "      <td>1651782</td>\n",
       "      <td>29 November 2002</td>\n",
       "      <td>10</td>\n",
       "      <td>Exquisite</td>\n",
       "      <td>They don't make 'em like this anymore. Marvelo...</td>\n",
       "      <td>99348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24187</th>\n",
       "      <td>24187</td>\n",
       "      <td>1001790</td>\n",
       "      <td>13 October 2002</td>\n",
       "      <td>10</td>\n",
       "      <td>Marvelous picture of love and respect for trad...</td>\n",
       "      <td>Marvelous picture of love and respect for trad...</td>\n",
       "      <td>99348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24188 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    userId        reviewDate  reviewScore  \\\n",
       "0               0   2509775  26 November 2003           10   \n",
       "1               1   1898687  10 February 2006           10   \n",
       "2               2  16161013      24 July 2010           10   \n",
       "3               3   1005460   8 February 2001           10   \n",
       "4               4    997166    27 August 2002           10   \n",
       "...           ...       ...               ...          ...   \n",
       "24183       24183      6729      17 June 2003            9   \n",
       "24184       24184   2349843        9 May 2003           10   \n",
       "24185       24185   1945054   4 December 2002           10   \n",
       "24186       24186   1651782  29 November 2002           10   \n",
       "24187       24187   1001790   13 October 2002           10   \n",
       "\n",
       "                                             reviewTitle  \\\n",
       "0               Tied for the best movie I have ever seen   \n",
       "1          A classic piece of unforgettable film-making.   \n",
       "2                   Some birds aren't meant to be caged.   \n",
       "3                                    Prepare to be moved   \n",
       "4                            Shawshank Redeems Hollywood   \n",
       "...                                                  ...   \n",
       "24183                                Changed my mind ...   \n",
       "24184                                      Read The Book   \n",
       "24185                  One of the best films in the 90's   \n",
       "24186                                          Exquisite   \n",
       "24187  Marvelous picture of love and respect for trad...   \n",
       "\n",
       "                                              userReview  movieId  \n",
       "0      Why do I want to write the 234th comment on Th...   111161  \n",
       "1      In its Oscar year, Shawshank Redemption (writt...   111161  \n",
       "2      The Shawshank Redemption is written and direct...   111161  \n",
       "3      I have never seen such an amazing film since I...   111161  \n",
       "4      Can Hollywood, usually creating things for ent...   111161  \n",
       "...                                                  ...      ...  \n",
       "24183  This post refers to the extended DVD special e...    99348  \n",
       "24184  This is specifically for Tom Hull...did you ha...    99348  \n",
       "24185  Dances with wolves is a very epic film.I must ...    99348  \n",
       "24186  They don't make 'em like this anymore. Marvelo...    99348  \n",
       "24187  Marvelous picture of love and respect for trad...    99348  \n",
       "\n",
       "[24188 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df = pd.read_csv(\"preprocessed_data.csv\")\n",
    "movie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract adjectives from the text\n",
    "def get_adjectives(text):\n",
    "    blob = TextBlob(text)\n",
    "    return [ word for (word,tag) in blob.tags if tag == \"JJ\"]\n",
    "\n",
    "movie_df['adjectives'] = movie_df['userReview'].apply(get_adjectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the reviews less than three words, drop this row to reduce the error\n",
    "def merge_list(list_):\n",
    "    return ' '.join(list_)\n",
    "\n",
    "movie_df['adjectives_merge'] = movie_df['adjectives'].apply(merge_list)\n",
    "\n",
    "movie_df['a_length'] = movie_df['adjectives'].apply(len)\n",
    "movie_df = movie_df[movie_df['a_length'] >= 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>userId</th>\n",
       "      <th>reviewDate</th>\n",
       "      <th>reviewScore</th>\n",
       "      <th>reviewTitle</th>\n",
       "      <th>userReview</th>\n",
       "      <th>movieId</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>adjectives_merge</th>\n",
       "      <th>a_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2509775</td>\n",
       "      <td>26 November 2003</td>\n",
       "      <td>10</td>\n",
       "      <td>Tied for the best movie I have ever seen</td>\n",
       "      <td>Why do I want to write the 234th comment on Th...</td>\n",
       "      <td>111161</td>\n",
       "      <td>[sure, many, other, simple, eloquent, only, ot...</td>\n",
       "      <td>sure many other simple eloquent only other fee...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1898687</td>\n",
       "      <td>10 February 2006</td>\n",
       "      <td>10</td>\n",
       "      <td>A classic piece of unforgettable film-making.</td>\n",
       "      <td>In its Oscar year, Shawshank Redemption (writt...</td>\n",
       "      <td>111161</td>\n",
       "      <td>[happy, good, all-time, huge, American, easy, ...</td>\n",
       "      <td>happy good all-time huge American easy mere ma...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>16161013</td>\n",
       "      <td>24 July 2010</td>\n",
       "      <td>10</td>\n",
       "      <td>Some birds aren't meant to be caged.</td>\n",
       "      <td>The Shawshank Redemption is written and direct...</td>\n",
       "      <td>111161</td>\n",
       "      <td>[tough, fellow, warden, much, unsure, several,...</td>\n",
       "      <td>tough fellow warden much unsure several mythic...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1005460</td>\n",
       "      <td>8 February 2001</td>\n",
       "      <td>10</td>\n",
       "      <td>Prepare to be moved</td>\n",
       "      <td>I have never seen such an amazing film since I...</td>\n",
       "      <td>111161</td>\n",
       "      <td>[amazing, great, different, simple, everlastin...</td>\n",
       "      <td>amazing great different simple everlasting out...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>997166</td>\n",
       "      <td>27 August 2002</td>\n",
       "      <td>10</td>\n",
       "      <td>Shawshank Redeems Hollywood</td>\n",
       "      <td>Can Hollywood, usually creating things for ent...</td>\n",
       "      <td>111161</td>\n",
       "      <td>[meticulous, due, Such, capable, undeniable, m...</td>\n",
       "      <td>meticulous due Such capable undeniable manager...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24183</th>\n",
       "      <td>24183</td>\n",
       "      <td>6729</td>\n",
       "      <td>17 June 2003</td>\n",
       "      <td>9</td>\n",
       "      <td>Changed my mind ...</td>\n",
       "      <td>This post refers to the extended DVD special e...</td>\n",
       "      <td>99348</td>\n",
       "      <td>[special, multiple, rare, fine, original, thea...</td>\n",
       "      <td>special multiple rare fine original theatrical...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24184</th>\n",
       "      <td>24184</td>\n",
       "      <td>2349843</td>\n",
       "      <td>9 May 2003</td>\n",
       "      <td>10</td>\n",
       "      <td>Read The Book</td>\n",
       "      <td>This is specifically for Tom Hull...did you ha...</td>\n",
       "      <td>99348</td>\n",
       "      <td>[very, same, other, western, outstanding, outs...</td>\n",
       "      <td>very same other western outstanding outstandin...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24185</th>\n",
       "      <td>24185</td>\n",
       "      <td>1945054</td>\n",
       "      <td>4 December 2002</td>\n",
       "      <td>10</td>\n",
       "      <td>One of the best films in the 90's</td>\n",
       "      <td>Dances with wolves is a very epic film.I must ...</td>\n",
       "      <td>99348</td>\n",
       "      <td>[epic, long, little, boring, good, great, nati...</td>\n",
       "      <td>epic long little boring good great native fabu...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24186</th>\n",
       "      <td>24186</td>\n",
       "      <td>1651782</td>\n",
       "      <td>29 November 2002</td>\n",
       "      <td>10</td>\n",
       "      <td>Exquisite</td>\n",
       "      <td>They don't make 'em like this anymore. Marvelo...</td>\n",
       "      <td>99348</td>\n",
       "      <td>[Marvelous, Great, entire, gorgeous, anti-Cost...</td>\n",
       "      <td>Marvelous Great entire gorgeous anti-Costner</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24187</th>\n",
       "      <td>24187</td>\n",
       "      <td>1001790</td>\n",
       "      <td>13 October 2002</td>\n",
       "      <td>10</td>\n",
       "      <td>Marvelous picture of love and respect for trad...</td>\n",
       "      <td>Marvelous picture of love and respect for trad...</td>\n",
       "      <td>99348</td>\n",
       "      <td>[Marvelous, new, violent, magnificent]</td>\n",
       "      <td>Marvelous new violent magnificent</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22749 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0    userId        reviewDate  reviewScore  \\\n",
       "0               0   2509775  26 November 2003           10   \n",
       "1               1   1898687  10 February 2006           10   \n",
       "2               2  16161013      24 July 2010           10   \n",
       "3               3   1005460   8 February 2001           10   \n",
       "4               4    997166    27 August 2002           10   \n",
       "...           ...       ...               ...          ...   \n",
       "24183       24183      6729      17 June 2003            9   \n",
       "24184       24184   2349843        9 May 2003           10   \n",
       "24185       24185   1945054   4 December 2002           10   \n",
       "24186       24186   1651782  29 November 2002           10   \n",
       "24187       24187   1001790   13 October 2002           10   \n",
       "\n",
       "                                             reviewTitle  \\\n",
       "0               Tied for the best movie I have ever seen   \n",
       "1          A classic piece of unforgettable film-making.   \n",
       "2                   Some birds aren't meant to be caged.   \n",
       "3                                    Prepare to be moved   \n",
       "4                            Shawshank Redeems Hollywood   \n",
       "...                                                  ...   \n",
       "24183                                Changed my mind ...   \n",
       "24184                                      Read The Book   \n",
       "24185                  One of the best films in the 90's   \n",
       "24186                                          Exquisite   \n",
       "24187  Marvelous picture of love and respect for trad...   \n",
       "\n",
       "                                              userReview  movieId  \\\n",
       "0      Why do I want to write the 234th comment on Th...   111161   \n",
       "1      In its Oscar year, Shawshank Redemption (writt...   111161   \n",
       "2      The Shawshank Redemption is written and direct...   111161   \n",
       "3      I have never seen such an amazing film since I...   111161   \n",
       "4      Can Hollywood, usually creating things for ent...   111161   \n",
       "...                                                  ...      ...   \n",
       "24183  This post refers to the extended DVD special e...    99348   \n",
       "24184  This is specifically for Tom Hull...did you ha...    99348   \n",
       "24185  Dances with wolves is a very epic film.I must ...    99348   \n",
       "24186  They don't make 'em like this anymore. Marvelo...    99348   \n",
       "24187  Marvelous picture of love and respect for trad...    99348   \n",
       "\n",
       "                                              adjectives  \\\n",
       "0      [sure, many, other, simple, eloquent, only, ot...   \n",
       "1      [happy, good, all-time, huge, American, easy, ...   \n",
       "2      [tough, fellow, warden, much, unsure, several,...   \n",
       "3      [amazing, great, different, simple, everlastin...   \n",
       "4      [meticulous, due, Such, capable, undeniable, m...   \n",
       "...                                                  ...   \n",
       "24183  [special, multiple, rare, fine, original, thea...   \n",
       "24184  [very, same, other, western, outstanding, outs...   \n",
       "24185  [epic, long, little, boring, good, great, nati...   \n",
       "24186  [Marvelous, Great, entire, gorgeous, anti-Cost...   \n",
       "24187             [Marvelous, new, violent, magnificent]   \n",
       "\n",
       "                                        adjectives_merge  a_length  \n",
       "0      sure many other simple eloquent only other fee...        16  \n",
       "1      happy good all-time huge American easy mere ma...        61  \n",
       "2      tough fellow warden much unsure several mythic...        36  \n",
       "3      amazing great different simple everlasting out...        10  \n",
       "4      meticulous due Such capable undeniable manager...        78  \n",
       "...                                                  ...       ...  \n",
       "24183  special multiple rare fine original theatrical...        26  \n",
       "24184  very same other western outstanding outstandin...         8  \n",
       "24185  epic long little boring good great native fabu...        12  \n",
       "24186       Marvelous Great entire gorgeous anti-Costner         5  \n",
       "24187                  Marvelous new violent magnificent         4  \n",
       "\n",
       "[22749 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_df.to_csv('data_adjs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split.\n",
    "train, test = train_test_split(movie_df['adjectives_merge'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function get_emotion to get the text emtions\n",
    "def get_emotion(sentence):\n",
    "    e = te.get_emotion(sentence)\n",
    "    return max(e, key = e.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjectives_merge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16276</th>\n",
       "      <td>British in-between former tragic alcohol-drive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>serial negative directorial dark clinically ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23048</th>\n",
       "      <td>hilarious graphic excessive hysterical sacrile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5694</th>\n",
       "      <td>many excellent perfectionist same nervous impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12505</th>\n",
       "      <td>sure perfect fantastic perfect beautiful emoti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12889</th>\n",
       "      <td>great many last only slight obvious different ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22942</th>\n",
       "      <td>soft many amazing many phenomenal strong fit l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5870</th>\n",
       "      <td>great historical second great wartime first we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>remarkable friendly big American congressional...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16893</th>\n",
       "      <td>interesting worth allow high true true similar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20474 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        adjectives_merge\n",
       "16276  British in-between former tragic alcohol-drive...\n",
       "1683   serial negative directorial dark clinically ex...\n",
       "23048  hilarious graphic excessive hysterical sacrile...\n",
       "5694   many excellent perfectionist same nervous impo...\n",
       "12505  sure perfect fantastic perfect beautiful emoti...\n",
       "...                                                  ...\n",
       "12889  great many last only slight obvious different ...\n",
       "22942  soft many amazing many phenomenal strong fit l...\n",
       "5870   great historical second great wartime first we...\n",
       "983    remarkable friendly big American congressional...\n",
       "16893  interesting worth allow high true true similar...\n",
       "\n",
       "[20474 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using get emotion package to get the sentiment label and add to test_df for eternal evaluation\n",
    "test_df = test.to_frame()\n",
    "test_df['label'] = test_df['adjectives_merge'].apply(get_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adjectives_merge</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21336</th>\n",
       "      <td>outstanding true digital investigative remarka...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12187</th>\n",
       "      <td>commented easy German 'evil Little little Germ...</td>\n",
       "      <td>Angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12796</th>\n",
       "      <td>Real Real terrible sole simple Wild hot ex-bab...</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14520</th>\n",
       "      <td>Sure outlandishly true adrenaline-pumping firs...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20162</th>\n",
       "      <td>main real real ethnic little common little oth...</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2398</th>\n",
       "      <td>male abnormal amazing early adoring male human...</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>outstanding stunning fantastic</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23733</th>\n",
       "      <td>original good same entertaining boring</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>perfect perfect perfect first much perfect wor...</td>\n",
       "      <td>Sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23924</th>\n",
       "      <td>amazing Great many magnificent</td>\n",
       "      <td>Happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2275 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        adjectives_merge  label\n",
       "21336  outstanding true digital investigative remarka...  Happy\n",
       "12187  commented easy German 'evil Little little Germ...  Angry\n",
       "12796  Real Real terrible sole simple Wild hot ex-bab...    Sad\n",
       "14520  Sure outlandishly true adrenaline-pumping firs...  Happy\n",
       "20162  main real real ethnic little common little oth...    Sad\n",
       "...                                                  ...    ...\n",
       "2398   male abnormal amazing early adoring male human...  Happy\n",
       "779                       outstanding stunning fantastic  Happy\n",
       "23733             original good same entertaining boring  Happy\n",
       "301    perfect perfect perfect first much perfect wor...    Sad\n",
       "23924                     amazing Great many magnificent  Happy\n",
       "\n",
       "[2275 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_kmean(train, test, num_clusters = 2, distance ='cosine_distance'):\n",
    "    \n",
    "    predict = None\n",
    "    \n",
    "    drop_lists = ['much', 'first', 'many', 'non', 'human', 'last', 'main', 'sure', 'anti', 'top', 'whole', \\\n",
    "                 'little', 'able', 'true', 'hard', 'final', 'full', 'second', 'entire', \\\n",
    "                 'long', 'year', 'big', 'small', 'several', \\\n",
    "                 'short', 'overall', 'next', 'obvious', \\\n",
    "                 'self', 'right', 'actual', \\\n",
    "                 'third', 'personal']\n",
    "    stop = list(stopwords.words('english')) + drop_lists\n",
    "    \n",
    "    tfidf_vect = TfidfVectorizer(stop_words=stop,\\\n",
    "                             min_df=3) \n",
    "\n",
    "    dtm= tfidf_vect.fit_transform(train)\n",
    "    \n",
    "    if distance == 'cosine_distance':\n",
    "    \n",
    "        clusterer = KMeansClusterer(num_clusters, \\\n",
    "                            cosine_distance, \\\n",
    "                            repeats=20)\n",
    "\n",
    "        clusters = clusterer.cluster(dtm.toarray(), \\\n",
    "                             assign_clusters=True)     \n",
    "        \n",
    "        centroids=np.array(clusterer.means())\n",
    "\n",
    "        sorted_centroids = centroids.argsort()[:, ::-1] \n",
    "\n",
    "        voc_lookup= tfidf_vect.get_feature_names()    \n",
    "\n",
    "        for i in range(num_clusters):\n",
    "\n",
    "            # get words with top 20 tf-idf weight in the centroid\n",
    "            top_words=[voc_lookup[word_index] \\\n",
    "                       for word_index in sorted_centroids[i, :20]]\n",
    "            print(\"Cluster %d:\\n %s \" % (i, \"; \".join(top_words)))\n",
    "\n",
    "        test_dtm = tfidf_vect.transform(test)\n",
    "\n",
    "        predict = [clusterer.classify(v) for v in test_dtm.toarray()]\n",
    "\n",
    "    elif distance == 'euclidean_distance':\n",
    "        clusterer = KMeans(n_clusters=num_clusters, n_init=20, random_state = 42)\\\n",
    "            .fit(dtm)\n",
    "        \n",
    "        clusters = clusterer.labels_.tolist()\n",
    "        \n",
    "        centroids = clusterer.cluster_centers_\n",
    "\n",
    "        sorted_centroids = centroids.argsort()[:, ::-1] \n",
    "\n",
    "        voc_lookup= tfidf_vect.get_feature_names()    \n",
    "\n",
    "        for i in range(num_clusters):\n",
    "\n",
    "            # get words with top 20 tf-idf weight in the centroid\n",
    "            top_words=[voc_lookup[word_index] \\\n",
    "                       for word_index in sorted_centroids[i, :20]]\n",
    "            print(\"Cluster %d:\\n %s \" % (i, \"; \".join(top_words)))\n",
    "\n",
    "        test_dtm = tfidf_vect.transform(test)\n",
    "        \n",
    "        predict = clusterer.predict(test_dtm)\n",
    "        \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_evaluate(pred_clusters, test_labels):\n",
    "\n",
    "    confusion_df = pd.DataFrame(list(zip(test_labels.values, pred_clusters)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "    \n",
    "    ct = pd.crosstab(index=confusion_df.cluster, columns=confusion_df.label)\n",
    "    ct_max = ct.idxmax(axis = 1)\n",
    "    cluster_dict={i:ct_max.iloc[i] for i in range(5)}\n",
    "\n",
    "    predicted_target=[cluster_dict[i] \\\n",
    "                      for i in pred_clusters]\n",
    "    print(metrics.classification_report\\\n",
    "      (test_labels, predicted_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/nltk/cluster/util.py:131: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - (numpy.dot(u, v) / (sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      " amazing; perfect; incredible; favorite; excellent; brilliant; fantastic; wonderful; great; good; beautiful; classic; awesome; special; different; favourite; funny; emotional; memorable; superb \n",
      "Cluster 1:\n",
      " great; good; wonderful; classic; funny; old; excellent; perfect; fantastic; different; brilliant; favorite; interesting; beautiful; special; new; original; bad; young; important \n",
      "Cluster 2:\n",
      " good; bad; great; boring; interesting; funny; old; high; excellent; wrong; classic; different; original; new; special; nice; stupid; brilliant; real; slow \n",
      "Cluster 3:\n",
      " real; great; good; old; young; different; bad; beautiful; new; american; excellent; brilliant; perfect; important; special; simple; realistic; wonderful; interesting; funny \n",
      "Cluster 4:\n",
      " old; young; new; different; beautiful; original; classic; black; powerful; white; emotional; american; important; simple; funny; high; special; interesting; great; wonderful \n"
     ]
    }
   ],
   "source": [
    "#Kmeans clustering results\n",
    "pred_clusters_kmean = cluster_kmean(train_df['adjectives_merge'], test_df['adjectives_merge'], num_clusters=5, distance='cosine_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.00      0.00      0.00        39\n",
      "        Fear       0.00      0.00      0.00       402\n",
      "       Happy       0.57      0.94      0.71      1252\n",
      "         Sad       0.38      0.26      0.31       321\n",
      "    Surprise       0.00      0.00      0.00       261\n",
      "\n",
      "    accuracy                           0.55      2275\n",
      "   macro avg       0.19      0.24      0.20      2275\n",
      "weighted avg       0.37      0.55      0.43      2275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Kmeans external_evaluation\n",
    "external_evaluate(pred_clusters_kmean, test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_lda(train, test_text, num_clusters=2):\n",
    "    \n",
    "    predict = None\n",
    "    \n",
    "    # add  your code\n",
    "    # LDA can only use raw term counts for LDA \n",
    "    drop_lists = ['much', 'first', 'many', 'non', 'human', 'last', 'main', 'sure', 'anti', 'top', 'whole', \\\n",
    "                 'little', 'able', 'true', 'hard', 'final', 'full', 'second', 'entire', \\\n",
    "                 'long', 'year', 'big', 'small', 'several', \\\n",
    "                 'short', 'overall', 'next', 'obvious', \\\n",
    "                 'self', 'right', 'actual', \\\n",
    "                 'third', 'personal']\n",
    "    stop = list(stopwords.words('english')) + drop_lists\n",
    "    \n",
    "    tf_vectorizer = CountVectorizer(stop_words=stop,\\\n",
    "                             min_df=3)\n",
    "    \n",
    "    tf = tf_vectorizer.fit_transform(train)\n",
    "    \n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    num_topics = num_clusters\n",
    "\n",
    "\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, \\\n",
    "                                    max_iter=30,verbose=1,\n",
    "                                    evaluate_every=1, n_jobs=-1,\n",
    "                                    random_state=0).fit(tf)\n",
    "    \n",
    "    num_top_words=20\n",
    "    \n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        # print out top 20 words per topic \n",
    "        words=[(tf_feature_names[i],'%.2f'%topic[i]) \\\n",
    "               for i in topic.argsort()[::-1][0:num_top_words]]\n",
    "        print(words)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    test_dtm = tf_vectorizer.transform(test_text)\n",
    "\n",
    "    lda_test = lda.transform(test_dtm.toarray())\n",
    "    \n",
    "    predict = pd.DataFrame(lda_test).idxmax(axis = 1).tolist()\n",
    "    \n",
    "    return predict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 30, perplexity: 1675.1435\n",
      "iteration: 2 of max_iter: 30, perplexity: 1630.6157\n",
      "iteration: 3 of max_iter: 30, perplexity: 1603.0338\n",
      "iteration: 4 of max_iter: 30, perplexity: 1582.3921\n",
      "iteration: 5 of max_iter: 30, perplexity: 1565.6308\n",
      "iteration: 6 of max_iter: 30, perplexity: 1552.0638\n",
      "iteration: 7 of max_iter: 30, perplexity: 1541.2786\n",
      "iteration: 8 of max_iter: 30, perplexity: 1532.5542\n",
      "iteration: 9 of max_iter: 30, perplexity: 1525.4775\n",
      "iteration: 10 of max_iter: 30, perplexity: 1519.8620\n",
      "iteration: 11 of max_iter: 30, perplexity: 1515.1126\n",
      "iteration: 12 of max_iter: 30, perplexity: 1511.2271\n",
      "iteration: 13 of max_iter: 30, perplexity: 1507.8874\n",
      "iteration: 14 of max_iter: 30, perplexity: 1505.0315\n",
      "iteration: 15 of max_iter: 30, perplexity: 1502.3972\n",
      "iteration: 16 of max_iter: 30, perplexity: 1500.1213\n",
      "iteration: 17 of max_iter: 30, perplexity: 1498.0960\n",
      "iteration: 18 of max_iter: 30, perplexity: 1496.3081\n",
      "iteration: 19 of max_iter: 30, perplexity: 1494.6778\n",
      "iteration: 20 of max_iter: 30, perplexity: 1493.2512\n",
      "iteration: 21 of max_iter: 30, perplexity: 1491.9136\n",
      "iteration: 22 of max_iter: 30, perplexity: 1490.7053\n",
      "iteration: 23 of max_iter: 30, perplexity: 1489.5049\n",
      "iteration: 24 of max_iter: 30, perplexity: 1488.3032\n",
      "iteration: 25 of max_iter: 30, perplexity: 1487.2767\n",
      "iteration: 26 of max_iter: 30, perplexity: 1486.2493\n",
      "iteration: 27 of max_iter: 30, perplexity: 1485.3895\n",
      "iteration: 28 of max_iter: 30, perplexity: 1484.6069\n",
      "iteration: 29 of max_iter: 30, perplexity: 1483.8295\n",
      "iteration: 30 of max_iter: 30, perplexity: 1483.0715\n",
      "Topic 0:\n",
      "[('real', '1446.51'), ('bad', '1344.32'), ('good', '1296.33'), ('different', '959.62'), ('original', '922.31'), ('old', '901.45'), ('japanese', '713.50'), ('new', '631.43'), ('french', '551.95'), ('wrong', '532.74'), ('interesting', '482.70'), ('high', '447.66'), ('dead', '443.74'), ('important', '432.43'), ('fi', '406.19'), ('sci', '401.19'), ('poor', '389.59'), ('stupid', '380.63'), ('young', '376.37'), ('boring', '347.17')]\n",
      "\n",
      "\n",
      "Topic 1:\n",
      "[('german', '937.19'), ('beautiful', '749.98'), ('real', '707.37'), ('powerful', '654.66'), ('great', '492.07'), ('excellent', '490.22'), ('brilliant', '408.78'), ('incredible', '346.28'), ('war', '341.53'), ('realistic', '334.73'), ('fantastic', '331.94'), ('memorable', '329.51'), ('classic', '323.79'), ('young', '292.41'), ('dark', '280.38'), ('cinematic', '278.97'), ('well', '277.69'), ('strong', '271.62'), ('effective', '268.38'), ('tragic', '265.10')]\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "[('american', '1911.61'), ('black', '1745.29'), ('good', '1601.13'), ('white', '1541.19'), ('great', '1051.45'), ('old', '1021.30'), ('western', '883.19'), ('british', '864.19'), ('young', '822.48'), ('real', '623.23'), ('classic', '552.73'), ('political', '441.84'), ('new', '429.94'), ('famous', '400.84'), ('rich', '392.34'), ('bad', '366.49'), ('early', '360.41'), ('funny', '359.93'), ('italian', '341.26'), ('local', '339.46')]\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "[('great', '9090.55'), ('good', '7961.67'), ('perfect', '1976.55'), ('amazing', '1773.93'), ('excellent', '1661.10'), ('funny', '1533.18'), ('bad', '1395.78'), ('wonderful', '1393.81'), ('real', '1330.13'), ('favorite', '1317.81'), ('classic', '1260.78'), ('fantastic', '1160.65'), ('brilliant', '1090.99'), ('special', '1068.01'), ('beautiful', '1056.34'), ('interesting', '998.55'), ('different', '965.36'), ('new', '904.49'), ('incredible', '866.98'), ('original', '794.16')]\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "[('old', '1256.09'), ('young', '1255.55'), ('new', '1143.94'), ('great', '1020.32'), ('different', '910.39'), ('emotional', '878.50'), ('silent', '855.94'), ('simple', '821.14'), ('real', '800.76'), ('perfect', '749.68'), ('beautiful', '738.02'), ('visual', '703.91'), ('cinematic', '669.44'), ('powerful', '668.10'), ('important', '624.72'), ('modern', '566.19'), ('wonderful', '548.43'), ('special', '525.98'), ('strong', '509.70'), ('brilliant', '438.53')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LDA clustering results\n",
    "pred_clusters_lda = cluster_lda(train_df['adjectives_merge'], test_df['adjectives_merge'], num_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.00      0.00      0.00        39\n",
      "        Fear       0.00      0.00      0.00       402\n",
      "       Happy       0.55      1.00      0.71      1252\n",
      "         Sad       0.00      0.00      0.00       321\n",
      "    Surprise       0.00      0.00      0.00       261\n",
      "\n",
      "    accuracy                           0.55      2275\n",
      "   macro avg       0.11      0.20      0.14      2275\n",
      "weighted avg       0.30      0.55      0.39      2275\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# LDA external_evaluation\n",
    "external_evaluate(pred_clusters_lda, test_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual drop lists\n",
    "\n",
    "drop_lists1 = ['much', 'first', 'many', 'non', 'human', 'last', 'main', 'sure', 'anti', 'top', 'whole']\n",
    "drop_lists2 = ['little', 'able', 'true', 'hard', 'final', 'full', 'second', 'entire']\n",
    "drop_lists3 = ['long', 'year', 'big', 'small', 'several']\n",
    "drop_lists4 = ['short', 'overall', 'next', 'obvious']\n",
    "drop_lists5 = ['self', 'right', 'actual']\n",
    "drop_lists6 = ['third', 'personal']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
